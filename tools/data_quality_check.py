#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬
æ”¯æŒæ£€æŸ¥CSVã€JSONã€Excelã€Parquetç­‰æ ¼å¼æ–‡ä»¶çš„æ•°æ®è´¨é‡
"""
import pandas as pd
import json
import argparse
from pathlib import Path
import logging
import numpy as np
from typing import Dict, List, Optional, Union
import pyarrow.parquet as pq
import psutil
import gc
import os

# æ€§èƒ½ä¼˜åŒ–é…ç½®
MEMORY_CHECK_INTERVAL = 100 * 1024 * 1024  # æ¯å¤„ç†100MBæ£€æŸ¥ä¸€æ¬¡å†…å­˜
MEMORY_THRESHOLD = 80  # å†…å­˜ä½¿ç”¨ç‡è­¦å‘Šé˜ˆå€¼ï¼ˆç™¾åˆ†æ•°ï¼‰
BATCH_SIZE = 10000  # é»˜è®¤æ‰¹å¤„ç†å¤§å°
BUFFER_SIZE = 8192 * 1024  # 8MBæ–‡ä»¶ç¼“å†²åŒºå¤§å°

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def get_memory_usage():
    """è·å–å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    total_memory = psutil.virtual_memory().total
    memory_percent = (memory_info.rss / total_memory) * 100
    return memory_info.rss / (1024 * 1024), memory_percent

def check_memory_usage():
    """æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¦‚æœè¶…è¿‡é˜ˆå€¼åˆ™å‘å‡ºè­¦å‘Š"""
    memory_usage, memory_percent = get_memory_usage()
    if memory_percent > MEMORY_THRESHOLD:
        logger.warning(f"å†…å­˜ä½¿ç”¨è¶…è¿‡é˜ˆå€¼: {memory_usage:.2f}MB ({memory_percent:.1f}%)")
        gc.collect()
    return memory_usage, memory_percent

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥ç±»"""
    
    SUPPORTED_FORMATS = {
        '.csv': 'é€—å·åˆ†éš”å€¼æ–‡ä»¶',
        '.json': 'JSONæ–‡ä»¶',
        '.parquet': 'Parquetæ–‡ä»¶',
        '.xlsx': 'Excelæ–‡ä»¶',
        '.xls': 'Excelæ–‡ä»¶',
        '.txt': 'æ–‡æœ¬æ–‡ä»¶'  # æ–°å¢æ”¯æŒ
    }
    
    def __init__(self):
        self.data = None
        self.file_path = None
        self.file_type = None
        self.error_stats = {"errors": [], "error_count": 0}
        
    def record_error(self, error_type: str, message: str):
        """è®°å½•é”™è¯¯ä¿¡æ¯"""
        self.error_stats["errors"].append({
            "type": error_type,
            "message": message,
            "file": str(self.file_path)
        })
        self.error_stats["error_count"] += 1
        logger.error(f"{error_type}: {message}")
        
    def is_supported_format(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ”¯æŒ"""
        return file_path.suffix.lower() in self.SUPPORTED_FORMATS
        
    def load_file(self, file_path: Union[str, Path]) -> bool:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        self.file_path = Path(file_path)
        self.file_type = self.file_path.suffix.lower()
        
        if not self.file_path.exists():
            self.record_error("FileNotFound", f"æ–‡ä»¶ä¸å­˜åœ¨: {self.file_path}")
            return False
            
        if not self.is_supported_format(self.file_path):
            self.record_error("UnsupportedFormat", f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.file_type}")
            logger.info(f"æ”¯æŒçš„æ ¼å¼: {', '.join(self.SUPPORTED_FORMATS.keys())}")
            return False
        
        try:
            if self.file_type == '.csv':
                self.data = pd.read_csv(file_path, encoding='utf-8')
            elif self.file_type == '.txt':
                # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
                for sep in [',', '\t', '|', ';']:
                    try:
                        self.data = pd.read_csv(file_path, sep=sep, encoding='utf-8')
                        if len(self.data.columns) > 1:  # å¦‚æœæˆåŠŸè§£æå‡ºå¤šåˆ—ï¼Œè¯´æ˜æ‰¾åˆ°äº†æ­£ç¡®çš„åˆ†éš”ç¬¦
                            break
                    except:
                        continue
            elif self.file_type == '.json':
                self.data = pd.read_json(file_path, encoding='utf-8')
            elif self.file_type == '.parquet':
                self.data = pq.read_table(file_path).to_pandas()
            elif self.file_type in ['.xlsx', '.xls']:
                self.data = pd.read_excel(file_path)
                
            if self.data is None:
                self.record_error("LoadError", f"æ— æ³•è§£ææ–‡ä»¶å†…å®¹: {file_path}")
                return False
                
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            check_memory_usage()
                
            logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶: {file_path}")
            logger.info(f"æ•°æ®å¤§å°: {len(self.data)} è¡Œ, {len(self.data.columns)} åˆ—")
            return True
            
        except Exception as e:
            self.record_error("LoadError", f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return False
    
    def check_basic_info(self) -> Dict:
        """æ£€æŸ¥åŸºæœ¬ä¿¡æ¯"""
        return {
            "æ–‡ä»¶å": self.file_path.name,
            "æ–‡ä»¶ç±»å‹": self.file_type,
            "è¡Œæ•°": len(self.data),
            "åˆ—æ•°": len(self.data.columns),
            "åˆ—å": self.data.columns.tolist(),
            "å†…å­˜å ç”¨(MB)": round(self.data.memory_usage(deep=True).sum() / 1024 / 1024, 2)
        }
    
    def check_null_values(self) -> Dict:
        """æ£€æŸ¥ç©ºå€¼"""
        null_stats = {}
        for col in self.data.columns:
            null_count = self.data[col].isnull().sum()
            null_stats[col] = {
                "ç©ºå€¼æ•°é‡": int(null_count),
                "ç©ºå€¼æ¯”ä¾‹": round(float(null_count / len(self.data) * 100), 2)
            }
        return null_stats
    
    def check_duplicates(self) -> Dict:
        """æ£€æŸ¥é‡å¤å€¼"""
        # å…¨è¡Œé‡å¤
        full_duplicates = self.data.duplicated().sum()
        # å•åˆ—é‡å¤
        column_duplicates = {}
        for col in self.data.columns:
            dup_count = self.data[col].duplicated().sum()
            column_duplicates[col] = {
                "é‡å¤å€¼æ•°é‡": int(dup_count),
                "é‡å¤å€¼æ¯”ä¾‹": round(float(dup_count / len(self.data) * 100), 2)
            }
            
        return {
            "å…¨è¡Œé‡å¤æ•°": int(full_duplicates),
            "å…¨è¡Œé‡å¤æ¯”ä¾‹": round(float(full_duplicates / len(self.data) * 100), 2),
            "å•åˆ—é‡å¤ç»Ÿè®¡": column_duplicates
        }
    
    def check_data_types(self) -> Dict:
        """æ£€æŸ¥æ•°æ®ç±»å‹"""
        type_stats = {}
        for col in self.data.columns:
            type_stats[col] = {
                "æ•°æ®ç±»å‹": str(self.data[col].dtype),
                "éç©ºå”¯ä¸€å€¼æ•°é‡": int(self.data[col].nunique()),
                "ç¤ºä¾‹å€¼": str(self.data[col].iloc[0]) if len(self.data) > 0 else None
            }
        return type_stats
    
    def check_numeric_stats(self) -> Dict:
        """æ£€æŸ¥æ•°å€¼ç»Ÿè®¡"""
        numeric_stats = {}
        numeric_columns = self.data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_columns:
            stats = self.data[col].describe()
            numeric_stats[col] = {
                "æœ€å°å€¼": float(stats['min']),
                "æœ€å¤§å€¼": float(stats['max']),
                "å¹³å‡å€¼": float(stats['mean']),
                "ä¸­ä½æ•°": float(stats['50%']),
                "æ ‡å‡†å·®": float(stats['std'])
            }
        return numeric_stats
    
    def check_string_length(self) -> Dict:
        """æ£€æŸ¥å­—ç¬¦ä¸²é•¿åº¦"""
        string_stats = {}
        string_columns = self.data.select_dtypes(include=['object']).columns
        
        for col in string_columns:
            lengths = self.data[col].astype(str).str.len()
            string_stats[col] = {
                "æœ€çŸ­é•¿åº¦": int(lengths.min()),
                "æœ€é•¿é•¿åº¦": int(lengths.max()),
                "å¹³å‡é•¿åº¦": round(float(lengths.mean()), 2)
            }
        return string_stats
    
    def run_all_checks(self) -> Dict:
        """è¿è¡Œæ‰€æœ‰æ£€æŸ¥"""
        if self.data is None:
            return {"error": "æœªåŠ è½½æ•°æ®æ–‡ä»¶"}
            
        try:
            results = {
                "åŸºæœ¬ä¿¡æ¯": self.check_basic_info(),
                "ç©ºå€¼æ£€æŸ¥": self.check_null_values(),
                "é‡å¤å€¼æ£€æŸ¥": self.check_duplicates(),
                "æ•°æ®ç±»å‹æ£€æŸ¥": self.check_data_types(),
                "æ•°å€¼ç»Ÿè®¡": self.check_numeric_stats(),
                "å­—ç¬¦ä¸²é•¿åº¦ç»Ÿè®¡": self.check_string_length(),
                "é”™è¯¯ç»Ÿè®¡": self.error_stats
            }
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨
            check_memory_usage()
            
            return results
            
        except Exception as e:
            self.record_error("CheckError", f"æ‰§è¡Œæ£€æŸ¥æ—¶å‡ºé”™: {str(e)}")
            return {"error": str(e), "é”™è¯¯ç»Ÿè®¡": self.error_stats}

    def get_single_record(self, file_path: Union[str, Path]) -> Dict:
        """åªè¯»å–ä¸€æ¡è®°å½•æ ·ä¾‹ï¼ˆç¬¬äºŒè¡Œæ•°æ®ï¼‰"""
        self.file_path = Path(file_path)
        self.file_type = self.file_path.suffix.lower()
        
        if not self.file_path.exists():
            return {"error": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
        if not self.is_supported_format(self.file_path):
            return {"error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {self.file_type}"}
        
        try:
            # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„è¯»å–æ–¹å¼
            if self.file_type == '.csv':
                # è¯»å–å‰ä¸¤è¡Œï¼Œå–ç¬¬äºŒè¡Œ
                sample = pd.read_csv(file_path, nrows=2).iloc[1]
            elif self.file_type == '.txt':
                # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
                for sep in [',', '\t', '|', ';']:
                    try:
                        sample = pd.read_csv(file_path, sep=sep, nrows=2)
                        if len(sample.columns) > 1:  # å¦‚æœæˆåŠŸè§£æå‡ºå¤šåˆ—
                            sample = sample.iloc[1]
                            break
                    except:
                        continue
            elif self.file_type == '.json':
                # JSONæ–‡ä»¶ç‰¹æ®Šå¤„ç†ï¼šè¯»å–ç¬¬ä¸€ä¸ªéç©ºè®°å½•
                with open(file_path, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and line not in ['[', ']', '{', '}']:
                            try:
                                data = json.loads(line)
                                if isinstance(data, dict):
                                    sample = pd.Series(data)
                                elif isinstance(data, list):
                                    sample = pd.Series(data[0] if data else {})
                                break
                            except:
                                continue
            elif self.file_type == '.parquet':
                sample = pq.read_table(file_path, num_rows=2).to_pandas().iloc[1]
            elif self.file_type in ['.xlsx', '.xls']:
                sample = pd.read_excel(file_path, nrows=2).iloc[1]
                
            if sample is None or len(sample) == 0:
                return {"error": "æ–‡ä»¶ä¸ºç©ºæˆ–æ— æ³•è¯»å–æ•°æ®"}
                
            # è½¬æ¢æˆå€¼çš„åˆ—è¡¨
            values = []
            for value in sample:
                if pd.isna(value):
                    values.append(None)
                elif isinstance(value, (np.int64, np.int32)):
                    values.append(int(value))
                elif isinstance(value, (np.float64, np.float32)):
                    values.append(float(value))
                else:
                    values.append(str(value))
                    
            return {"æ ·ä¾‹è®°å½•": values}
            
        except Exception as e:
            return {"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}

    def get_summary_stats(self) -> Dict:
        """è·å–æ•°æ®è´¨é‡æ‘˜è¦ç»Ÿè®¡"""
        if self.data is None:
            return {"error": "æœªåŠ è½½æ•°æ®æ–‡ä»¶"}
            
        # åŸºæœ¬ä¿¡æ¯
        total_rows = len(self.data)
        total_cols = len(self.data.columns)
        
        # ç©ºå€¼ç»Ÿè®¡
        null_counts = self.data.isnull().sum()
        cols_with_nulls = sum(null_counts > 0)
        total_nulls = null_counts.sum()
        
        # é‡å¤å€¼ç»Ÿè®¡
        full_duplicates = self.data.duplicated().sum()
        
        # æ•°æ®ç±»å‹ç»Ÿè®¡
        dtype_counts = self.data.dtypes.value_counts().to_dict()
        dtype_counts = {str(k): int(v) for k, v in dtype_counts.items()}
        
        # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆé’ˆå¯¹æ•°å€¼åˆ—ï¼‰
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns
        outlier_stats = {}
        if len(numeric_cols) > 0:
            for col in numeric_cols:
                q1 = self.data[col].quantile(0.25)
                q3 = self.data[col].quantile(0.75)
                iqr = q3 - q1
                outliers = self.data[
                    (self.data[col] < (q1 - 1.5 * iqr)) | 
                    (self.data[col] > (q3 + 1.5 * iqr))
                ]
                if len(outliers) > 0:
                    outlier_stats[col] = len(outliers)
        
        return {
            "æ•°æ®è§„æ¨¡": {
                "æ€»è¡Œæ•°": total_rows,
                "æ€»åˆ—æ•°": total_cols,
                "æ–‡ä»¶å¤§å°(MB)": round(self.data.memory_usage(deep=True).sum() / 1024 / 1024, 2)
            },
            "æ•°æ®å®Œæ•´æ€§": {
                "å«ç©ºå€¼çš„åˆ—æ•°": int(cols_with_nulls),
                "ç©ºå€¼æ€»æ•°": int(total_nulls),
                "ç©ºå€¼å æ¯”": round(float(total_nulls) / (total_rows * total_cols) * 100, 2)
            },
            "æ•°æ®é‡å¤æ€§": {
                "é‡å¤è¡Œæ•°": int(full_duplicates),
                "é‡å¤ç‡": round(float(full_duplicates) / total_rows * 100, 2)
            },
            "æ•°æ®ç±»å‹åˆ†å¸ƒ": dtype_counts,
            "å¼‚å¸¸å€¼ç»Ÿè®¡": {
                "å«å¼‚å¸¸å€¼çš„åˆ—æ•°": len(outlier_stats),
                "å¼‚å¸¸å€¼è¯¦æƒ…": outlier_stats
            }
        }

def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    help_text = """
æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·ä½¿ç”¨è¯´æ˜ï¼š

åŠŸèƒ½ï¼š
    æ£€æŸ¥CSVã€Excelã€JSONã€Parquetç­‰æ ¼å¼æ–‡ä»¶çš„æ•°æ®è´¨é‡ï¼Œæ”¯æŒå•æ–‡ä»¶å’Œç›®å½•æ‰¹é‡å¤„ç†ã€‚

æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š
    - .csv  : CSVæ–‡ä»¶
    - .xlsx : Excelæ–‡ä»¶
    - .xls  : Excelæ–‡ä»¶
    - .json : JSONæ–‡ä»¶
    - .parquet : Parquetæ–‡ä»¶
    - .txt  : æ–‡æœ¬æ–‡ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ«åˆ†éš”ç¬¦ï¼‰

å‚æ•°è¯´æ˜ï¼š
    path          è¦æ£€æŸ¥çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„
    -o, --output  æŒ‡å®šè¾“å‡ºç»“æœçš„JSONæ–‡ä»¶è·¯å¾„
    -d, --detail  æ˜¾ç¤ºè¯¦ç»†çš„æ£€æŸ¥ç»“æœï¼ˆä¸èƒ½ä¸-såŒæ—¶ä½¿ç”¨ï¼‰
    -s, --sample  åªæ˜¾ç¤ºéšæœºæ ·ä¾‹è®°å½•ï¼Œä¸è¿›è¡Œæ•°æ®åˆ†æ
    -f, --format  ä½¿ç”¨æ ¼å¼åŒ–è¾“å‡ºï¼ˆæ›´æ˜“è¯»çš„æ–‡æœ¬æ ¼å¼ï¼‰
    -h, --help    æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯

è¾“å‡ºæ ¼å¼ï¼š
    é»˜è®¤è¾“å‡ºJSONæ ¼å¼ï¼Œä½¿ç”¨ -f å‚æ•°å¯ä»¥è¾“å‡ºæ ¼å¼åŒ–çš„æ–‡æœ¬æŠ¥å‘Šã€‚
    å¦‚æœè¾“å‡ºæ–‡ä»¶æ‰©å±•åä¸º.txtï¼Œå°†è‡ªåŠ¨ä½¿ç”¨æ ¼å¼åŒ–è¾“å‡ºã€‚

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # æ£€æŸ¥å•ä¸ªæ–‡ä»¶
    python data_quality_check.py data.xlsx

    # æ£€æŸ¥æ•´ä¸ªç›®å½•
    python data_quality_check.py ./data_directory

    # ä¿å­˜æ£€æŸ¥ç»“æœåˆ°æ–‡ä»¶
    python data_quality_check.py data.csv -o result.json

    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    python data_quality_check.py data.xlsx -d

    # åªæ˜¾ç¤ºéšæœºæ ·ä¾‹è®°å½•
    python data_quality_check.py data.csv -s

    # ä½¿ç”¨æ ¼å¼åŒ–è¾“å‡º
    python data_quality_check.py data.xlsx -f

    # ä¿å­˜æ ¼å¼åŒ–æŠ¥å‘Š
    python data_quality_check.py data.xlsx -f -o report.txt

    # ç»„åˆä½¿ç”¨ï¼ˆæ³¨æ„ï¼š-sä¸èƒ½ä¸-dåŒæ—¶ä½¿ç”¨ï¼‰
    python data_quality_check.py data.xlsx -f -o report.txt
"""
    print(help_text)

def get_directory_summary(results: Dict) -> Dict:
    """è·å–ç›®å½•çš„æ•´ä½“ç»Ÿè®¡ä¿¡æ¯"""
    if not results:
        return {}
        
    # åˆå§‹åŒ–ç»Ÿè®¡æ•°æ®
    summary = {
        "æ–‡ä»¶ç»Ÿè®¡": {
            "æ€»æ–‡ä»¶æ•°": len(results) - (1 if "ç›®å½•ç»Ÿè®¡" in results else 0),  # æ’é™¤ç›®å½•ç»Ÿè®¡æœ¬èº«
            "å„ç±»å‹æ–‡ä»¶æ•°": {},
            "æ€»æ•°æ®é‡": {
                "æ€»è¡Œæ•°": 0,
                "å¹³å‡è¡Œæ•°": 0,
                "æœ€å¤§è¡Œæ•°": 0,
                "æœ€å°è¡Œæ•°": float('inf'),
                "æ€»å¤§å°(MB)": 0
            }
        },
        "æ•°æ®è´¨é‡ç»Ÿè®¡": {
            "ç©ºå€¼": {
                "æ€»ç©ºå€¼æ•°": 0,
                "å¹³å‡ç©ºå€¼ç‡": 0,
                "ç©ºå€¼æœ€å¤šçš„æ–‡ä»¶": "",
                "æœ€å¤§ç©ºå€¼ç‡": 0
            },
            "é‡å¤å€¼": {
                "æ€»é‡å¤è¡Œæ•°": 0,
                "å¹³å‡é‡å¤ç‡": 0,
                "é‡å¤æœ€å¤šçš„æ–‡ä»¶": "",
                "æœ€å¤§é‡å¤ç‡": 0
            },
            "å¼‚å¸¸å€¼": {
                "å«å¼‚å¸¸å€¼çš„æ–‡ä»¶æ•°": 0,
                "æ€»å¼‚å¸¸å€¼æ•°": 0
            }
        }
    }
    
    # æ”¶é›†ç»Ÿè®¡æ•°æ®
    total_rows = 0
    total_nulls_rate = 0
    total_duplicates_rate = 0
    
    for file_path, result in results.items():
        if file_path == "ç›®å½•ç»Ÿè®¡":  # è·³è¿‡ç›®å½•ç»Ÿè®¡
            continue
            
        # æ–‡ä»¶ç±»å‹ç»Ÿè®¡
        file_type = Path(file_path).suffix
        summary["æ–‡ä»¶ç»Ÿè®¡"]["å„ç±»å‹æ–‡ä»¶æ•°"][file_type] = summary["æ–‡ä»¶ç»Ÿè®¡"]["å„ç±»å‹æ–‡ä»¶æ•°"].get(file_type, 0) + 1
        
        # æå–åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
        if "æ•°æ®è§„æ¨¡" in result:
            rows = result["æ•°æ®è§„æ¨¡"]["æ€»è¡Œæ•°"]
            total_rows += rows
            summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æ€»è¡Œæ•°"] = total_rows
            summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å¤§è¡Œæ•°"] = max(summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å¤§è¡Œæ•°"], rows)
            summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å°è¡Œæ•°"] = min(summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å°è¡Œæ•°"], rows)
            summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æ€»å¤§å°(MB)"] += result["æ•°æ®è§„æ¨¡"]["æ–‡ä»¶å¤§å°(MB)"]
        
        # ç©ºå€¼ç»Ÿè®¡
        if "æ•°æ®å®Œæ•´æ€§" in result:
            null_rate = result["æ•°æ®å®Œæ•´æ€§"]["ç©ºå€¼å æ¯”"]
            total_nulls_rate += null_rate
            if null_rate > summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["ç©ºå€¼"]["æœ€å¤§ç©ºå€¼ç‡"]:
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["ç©ºå€¼"]["æœ€å¤§ç©ºå€¼ç‡"] = null_rate
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["ç©ºå€¼"]["ç©ºå€¼æœ€å¤šçš„æ–‡ä»¶"] = file_path
            summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["ç©ºå€¼"]["æ€»ç©ºå€¼æ•°"] += result["æ•°æ®å®Œæ•´æ€§"]["ç©ºå€¼æ€»æ•°"]
        
        # é‡å¤å€¼ç»Ÿè®¡
        if "æ•°æ®é‡å¤æ€§" in result:
            dup_rate = result["æ•°æ®é‡å¤æ€§"]["é‡å¤ç‡"]
            total_duplicates_rate += dup_rate
            if dup_rate > summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["é‡å¤å€¼"]["æœ€å¤§é‡å¤ç‡"]:
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["é‡å¤å€¼"]["æœ€å¤§é‡å¤ç‡"] = dup_rate
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["é‡å¤å€¼"]["é‡å¤æœ€å¤šçš„æ–‡ä»¶"] = file_path
            summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["é‡å¤å€¼"]["æ€»é‡å¤è¡Œæ•°"] += result["æ•°æ®é‡å¤æ€§"]["é‡å¤è¡Œæ•°"]
        
        # å¼‚å¸¸å€¼ç»Ÿè®¡
        if "å¼‚å¸¸å€¼ç»Ÿè®¡" in result:
            if result["å¼‚å¸¸å€¼ç»Ÿè®¡"]["å«å¼‚å¸¸å€¼çš„åˆ—æ•°"] > 0:
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["å¼‚å¸¸å€¼"]["å«å¼‚å¸¸å€¼çš„æ–‡ä»¶æ•°"] += 1
                summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["å¼‚å¸¸å€¼"]["æ€»å¼‚å¸¸å€¼æ•°"] += sum(result["å¼‚å¸¸å€¼ç»Ÿè®¡"]["å¼‚å¸¸å€¼è¯¦æƒ…"].values())
    
    # è®¡ç®—å¹³å‡å€¼
    file_count = len(results) - (1 if "ç›®å½•ç»Ÿè®¡" in results else 0)
    if file_count > 0:
        summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["å¹³å‡è¡Œæ•°"] = round(total_rows / file_count, 2)
        summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["ç©ºå€¼"]["å¹³å‡ç©ºå€¼ç‡"] = round(total_nulls_rate / file_count, 2)
        summary["æ•°æ®è´¨é‡ç»Ÿè®¡"]["é‡å¤å€¼"]["å¹³å‡é‡å¤ç‡"] = round(total_duplicates_rate / file_count, 2)
    
    # å¤„ç†æ²¡æœ‰æ‰¾åˆ°æ–‡ä»¶çš„æƒ…å†µ
    if summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å°è¡Œæ•°"] == float('inf'):
        summary["æ–‡ä»¶ç»Ÿè®¡"]["æ€»æ•°æ®é‡"]["æœ€å°è¡Œæ•°"] = 0
    
    return summary

def process_path(path: Union[str, Path], checker: DataQualityChecker, args) -> Dict:
    """å¤„ç†æ–‡ä»¶æˆ–ç›®å½•"""
    path = Path(path)
    results = {}
    
    if path.is_file():
        if args.sample:
            # åªè¯»å–ä¸€æ¡è®°å½•
            results[str(path)] = checker.get_single_record(path)
        else:
            # æ­£å¸¸çš„æ•°æ®è´¨é‡æ£€æŸ¥
            if checker.load_file(path):
                result = checker.run_all_checks() if args.detail else checker.get_summary_stats()
                results[str(path)] = result
    elif path.is_dir():
        # é€’å½’å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ”¯æŒæ ¼å¼çš„æ–‡ä»¶
        for file_path in path.rglob("*"):
            if file_path.is_file() and checker.is_supported_format(file_path):
                if args.sample:
                    # åªè¯»å–ä¸€æ¡è®°å½•
                    results[str(file_path)] = checker.get_single_record(file_path)
                else:
                    # æ­£å¸¸çš„æ•°æ®è´¨é‡æ£€æŸ¥
                    if checker.load_file(file_path):
                        result = checker.run_all_checks() if args.detail else checker.get_summary_stats()
                        results[str(file_path)] = result
        
        # å¦‚æœä¸æ˜¯æ ·ä¾‹æ¨¡å¼ï¼Œä¸”æœ‰ç»“æœï¼Œæ·»åŠ ç›®å½•ç»Ÿè®¡
        if not args.sample and results:
            results["ç›®å½•ç»Ÿè®¡"] = get_directory_summary(results)
    else:
        logger.error(f"è·¯å¾„ä¸å­˜åœ¨: {path}")
    
    return results

def format_number(num: float) -> str:
    """æ ¼å¼åŒ–æ•°å­—è¾“å‡º"""
    if isinstance(num, (int, np.integer)):
        return f"{num:,}"
    return f"{num:,.2f}"

def format_report(results: Dict, is_detail: bool = False) -> str:
    """æ ¼å¼åŒ–æŠ¥å‘Šè¾“å‡º
    Args:
        results: æ£€æŸ¥ç»“æœ
        is_detail: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    Returns:
        æ ¼å¼åŒ–åçš„æŠ¥å‘Šæ–‡æœ¬
    """
    report = []
    
    for file_path, result in results.items():
        if file_path == "ç›®å½•ç»Ÿè®¡":
            report.append("\n" + "=" * 80)
            report.append("ç›®å½•ç»Ÿè®¡æŠ¥å‘Š")
            report.append("=" * 80)
            
            # æ–‡ä»¶ç»Ÿè®¡
            file_stats = result["æ–‡ä»¶ç»Ÿè®¡"]
            report.append("\nğŸ“Š æ–‡ä»¶ç»Ÿè®¡")
            report.append(f"æ€»æ–‡ä»¶æ•°: {format_number(file_stats['æ€»æ–‡ä»¶æ•°'])} ä¸ª")
            report.append("\næ–‡ä»¶ç±»å‹åˆ†å¸ƒ:")
            for ftype, count in file_stats["å„ç±»å‹æ–‡ä»¶æ•°"].items():
                report.append(f"  {ftype}: {count} ä¸ª")
            
            # æ•°æ®é‡ç»Ÿè®¡
            data_stats = file_stats["æ€»æ•°æ®é‡"]
            report.append("\nğŸ“ˆ æ•°æ®è§„æ¨¡")
            report.append(f"æ€»è¡Œæ•°: {format_number(data_stats['æ€»è¡Œæ•°'])} è¡Œ")
            report.append(f"å¹³å‡è¡Œæ•°: {format_number(data_stats['å¹³å‡è¡Œæ•°'])} è¡Œ")
            report.append(f"æœ€å¤§è¡Œæ•°: {format_number(data_stats['æœ€å¤§è¡Œæ•°'])} è¡Œ")
            report.append(f"æœ€å°è¡Œæ•°: {format_number(data_stats['æœ€å°è¡Œæ•°'])} è¡Œ")
            report.append(f"æ€»å¤§å°: {format_number(data_stats['æ€»å¤§å°(MB)'])} MB")
            
            # æ•°æ®è´¨é‡ç»Ÿè®¡
            quality_stats = result["æ•°æ®è´¨é‡ç»Ÿè®¡"]
            report.append("\nğŸ” æ•°æ®è´¨é‡")
            
            # ç©ºå€¼ç»Ÿè®¡
            null_stats = quality_stats["ç©ºå€¼"]
            report.append("\nç©ºå€¼ç»Ÿè®¡:")
            report.append(f"  æ€»ç©ºå€¼æ•°: {format_number(null_stats['æ€»ç©ºå€¼æ•°'])}")
            report.append(f"  å¹³å‡ç©ºå€¼ç‡: {format_number(null_stats['å¹³å‡ç©ºå€¼ç‡'])}%")
            report.append(f"  æœ€å¤§ç©ºå€¼ç‡: {format_number(null_stats['æœ€å¤§ç©ºå€¼ç‡'])}%")
            report.append(f"  ç©ºå€¼æœ€å¤šçš„æ–‡ä»¶: {Path(null_stats['ç©ºå€¼æœ€å¤šçš„æ–‡ä»¶']).name}")
            
            # é‡å¤å€¼ç»Ÿè®¡
            dup_stats = quality_stats["é‡å¤å€¼"]
            report.append("\né‡å¤å€¼ç»Ÿè®¡:")
            report.append(f"  æ€»é‡å¤è¡Œæ•°: {format_number(dup_stats['æ€»é‡å¤è¡Œæ•°'])}")
            report.append(f"  å¹³å‡é‡å¤ç‡: {format_number(dup_stats['å¹³å‡é‡å¤ç‡'])}%")
            report.append(f"  æœ€å¤§é‡å¤ç‡: {format_number(dup_stats['æœ€å¤§é‡å¤ç‡'])}%")
            report.append(f"  é‡å¤æœ€å¤šçš„æ–‡ä»¶: {Path(dup_stats['é‡å¤æœ€å¤šçš„æ–‡ä»¶']).name}")
            
            # å¼‚å¸¸å€¼ç»Ÿè®¡
            outlier_stats = quality_stats["å¼‚å¸¸å€¼"]
            report.append("\nå¼‚å¸¸å€¼ç»Ÿè®¡:")
            report.append(f"  å«å¼‚å¸¸å€¼çš„æ–‡ä»¶æ•°: {format_number(outlier_stats['å«å¼‚å¸¸å€¼çš„æ–‡ä»¶æ•°'])}")
            report.append(f"  æ€»å¼‚å¸¸å€¼æ•°: {format_number(outlier_stats['æ€»å¼‚å¸¸å€¼æ•°'])}")
            
        else:
            # å•ä¸ªæ–‡ä»¶çš„ç»Ÿè®¡
            report.append("\n" + "-" * 80)
            report.append(f"æ–‡ä»¶: {Path(file_path).name}")
            report.append("-" * 80)
            
            if "error" in result:
                report.append(f"é”™è¯¯: {result['error']}")
                continue
            
            # åŸºæœ¬ä¿¡æ¯
            data_size = result["æ•°æ®è§„æ¨¡"]
            report.append(f"\nğŸ“Š æ•°æ®è§„æ¨¡")
            report.append(f"æ€»è¡Œæ•°: {format_number(data_size['æ€»è¡Œæ•°'])} è¡Œ")
            report.append(f"æ€»åˆ—æ•°: {format_number(data_size['æ€»åˆ—æ•°'])} åˆ—")
            report.append(f"æ–‡ä»¶å¤§å°: {format_number(data_size['æ–‡ä»¶å¤§å°(MB)'])} MB")
            
            # æ•°æ®å®Œæ•´æ€§
            completeness = result["æ•°æ®å®Œæ•´æ€§"]
            report.append(f"\nğŸ” æ•°æ®å®Œæ•´æ€§")
            report.append(f"å«ç©ºå€¼çš„åˆ—æ•°: {format_number(completeness['å«ç©ºå€¼çš„åˆ—æ•°'])}")
            report.append(f"ç©ºå€¼æ€»æ•°: {format_number(completeness['ç©ºå€¼æ€»æ•°'])}")
            report.append(f"ç©ºå€¼å æ¯”: {format_number(completeness['ç©ºå€¼å æ¯”'])}%")
            
            # æ•°æ®é‡å¤æ€§
            duplicates = result["æ•°æ®é‡å¤æ€§"]
            report.append(f"\nğŸ”„ æ•°æ®é‡å¤æ€§")
            report.append(f"é‡å¤è¡Œæ•°: {format_number(duplicates['é‡å¤è¡Œæ•°'])}")
            report.append(f"é‡å¤ç‡: {format_number(duplicates['é‡å¤ç‡'])}%")
            
            # æ•°æ®ç±»å‹åˆ†å¸ƒ
            report.append(f"\nğŸ“‹ æ•°æ®ç±»å‹åˆ†å¸ƒ")
            for dtype, count in result["æ•°æ®ç±»å‹åˆ†å¸ƒ"].items():
                report.append(f"{dtype}: {count} åˆ—")
            
            # å¼‚å¸¸å€¼ç»Ÿè®¡
            outliers = result["å¼‚å¸¸å€¼ç»Ÿè®¡"]
            report.append(f"\nâš ï¸ å¼‚å¸¸å€¼ç»Ÿè®¡")
            report.append(f"å«å¼‚å¸¸å€¼çš„åˆ—æ•°: {outliers['å«å¼‚å¸¸å€¼çš„åˆ—æ•°']}")
            if outliers['å¼‚å¸¸å€¼è¯¦æƒ…']:
                report.append("å¼‚å¸¸å€¼è¯¦æƒ…:")
                for col, count in outliers['å¼‚å¸¸å€¼è¯¦æƒ…'].items():
                    report.append(f"  {col}: {format_number(count)} ä¸ª")
            
            # æ ·ä¾‹è®°å½•
            if "æ ·ä¾‹è®°å½•" in result:
                report.append(f"\nğŸ“ éšæœºæ ·ä¾‹è®°å½•")
                for value in result["æ ·ä¾‹è®°å½•"]:
                    report.append(f"{value}")
    
    return "\n".join(report)

def main():
    parser = argparse.ArgumentParser(description="æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·", add_help=False)
    parser.add_argument('path', nargs='?', help="è¦æ£€æŸ¥çš„æ–‡ä»¶æˆ–ç›®å½•è·¯å¾„")
    parser.add_argument('--output', '-o', help="è¾“å‡ºç»“æœçš„JSONæ–‡ä»¶è·¯å¾„")
    parser.add_argument('--detail', '-d', action='store_true', help="æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆä¸èƒ½ä¸-såŒæ—¶ä½¿ç”¨ï¼‰")
    parser.add_argument('--sample', '-s', action='store_true', help="åªæ˜¾ç¤ºéšæœºæ ·ä¾‹è®°å½•ï¼Œä¸è¿›è¡Œæ•°æ®åˆ†æ")
    parser.add_argument('--help', '-h', action='store_true', help="æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    parser.add_argument('--format', '-f', action='store_true', help="æ˜¯å¦ä½¿ç”¨æ ¼å¼åŒ–è¾“å‡º")
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if args.help or not args.path:
        show_help()
        return
    
    # æ£€æŸ¥å‚æ•°å†²çª
    if args.sample and args.detail:
        logger.error("å‚æ•°é”™è¯¯ï¼š-sï¼ˆæ ·ä¾‹æ¨¡å¼ï¼‰ä¸èƒ½ä¸-dï¼ˆè¯¦ç»†æ¨¡å¼ï¼‰åŒæ—¶ä½¿ç”¨")
        return
    
    # åˆ›å»ºæ£€æŸ¥å™¨å®ä¾‹
    checker = DataQualityChecker()
    
    # å¤„ç†æ–‡ä»¶æˆ–ç›®å½•
    results = process_path(args.path, checker, args)
    
    # è¾“å‡ºç»“æœ
    if results:
        if args.output:
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ ¹æ®è¾“å‡ºæ–‡ä»¶æ‰©å±•åå†³å®šæ ¼å¼
            if args.format or output_path.suffix == '.txt':
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(format_report(results, args.detail))
            else:
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(results, f, ensure_ascii=False, indent=2)
            logger.info(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        else:
            if args.format:
                print(format_report(results, args.detail))
            else:
                print(json.dumps(results, ensure_ascii=False, indent=2))
    else:
        logger.error("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶")

if __name__ == "__main__":
    main() 